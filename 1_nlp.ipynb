{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1.nlp",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dreaming-world/learn_tensorflow_nlp/blob/master/1_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoBbcWyqAwZo",
        "colab_type": "code",
        "outputId": "56654bc9-16b6-4f9e-cd00-42a9a35a93d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# 安装 TensorFlow\n",
        "try:\n",
        "  # Colab only\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9N_OCj_Bp5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import gensim\n",
        "import numpy as np\n",
        "import jieba"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7mV2OsWA1D8",
        "colab_type": "code",
        "outputId": "e927f5b5-c2a6-4792-f3fb-41bec23b2613",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "sentence = [\n",
        "            '机器学习我的理解就是把各种原始的东西变成机器可以理解的东西，然后再用各种机器学习算法来做操作',\n",
        "            '网上大部分都是处理英文文本的资料，本文就以中文文本为例，将原始的文本经过预处理得到文本向量'\n",
        "]\n",
        "data = []\n",
        "for ele in sentence:\n",
        "  data.append(list(jieba.cut(ele)))\n",
        "print(data)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['机器', '学习', '我', '的', '理解', '就是', '把', '各种', '原始', '的', '东西', '变成', '机器', '可以', '理解', '的', '东西', '，', '然后', '再用', '各种', '机器', '学习', '算法', '来', '做', '操作'], ['网上', '大部分', '都', '是', '处理', '英文', '文本', '的', '资料', '，', '本文', '就', '以', '中文', '文本', '为例', '，', '将', '原始', '的', '文本', '经过', '预处理', '得到', '文本', '向量']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag0C-mHgq3H_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAY8Fs-3uMA_",
        "colab_type": "code",
        "outputId": "873cdc22-1182-480d-a027-7dfa95f1b1a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "tokenizer = Tokenizer(oov_token='UNK')\n",
        "tokenizer.fit_on_texts(data)\n",
        "print( tokenizer.word_counts)\n",
        "print( tokenizer.word_index)\n",
        "print( tokenizer.word_docs) \n",
        "print( tokenizer.index_docs)\n",
        "print( tokenizer.texts_to_sequences(data))\n",
        "print( tokenizer.texts_to_matrix(data))\n",
        "#sklearn保存方法，通常用来保存模型\n",
        "from sklearn.externals import joblib \n",
        "joblib.dump(tokenizer,'dataFile.pkl')  #模型保存\n",
        "tokenizer1 = joblib.load('dataFile.pkl') #模型载入\n",
        "print(tokenizer1.word_index)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([('机器', 3), ('学习', 2), ('我', 1), ('的', 5), ('理解', 2), ('就是', 1), ('把', 1), ('各种', 2), ('原始', 2), ('东西', 2), ('变成', 1), ('可以', 1), ('，', 3), ('然后', 1), ('再用', 1), ('算法', 1), ('来', 1), ('做', 1), ('操作', 1), ('网上', 1), ('大部分', 1), ('都', 1), ('是', 1), ('处理', 1), ('英文', 1), ('文本', 4), ('资料', 1), ('本文', 1), ('就', 1), ('以', 1), ('中文', 1), ('为例', 1), ('将', 1), ('经过', 1), ('预处理', 1), ('得到', 1), ('向量', 1)])\n",
            "{'UNK': 1, '的': 2, '文本': 3, '机器': 4, '，': 5, '学习': 6, '理解': 7, '各种': 8, '原始': 9, '东西': 10, '我': 11, '就是': 12, '把': 13, '变成': 14, '可以': 15, '然后': 16, '再用': 17, '算法': 18, '来': 19, '做': 20, '操作': 21, '网上': 22, '大部分': 23, '都': 24, '是': 25, '处理': 26, '英文': 27, '资料': 28, '本文': 29, '就': 30, '以': 31, '中文': 32, '为例': 33, '将': 34, '经过': 35, '预处理': 36, '得到': 37, '向量': 38}\n",
            "defaultdict(<class 'int'>, {'算法': 1, '理解': 1, '学习': 1, '，': 2, '的': 2, '机器': 1, '原始': 2, '做': 1, '就是': 1, '我': 1, '操作': 1, '然后': 1, '各种': 1, '再用': 1, '变成': 1, '来': 1, '东西': 1, '可以': 1, '把': 1, '都': 1, '预处理': 1, '将': 1, '是': 1, '中文': 1, '得到': 1, '网上': 1, '文本': 1, '为例': 1, '就': 1, '向量': 1, '大部分': 1, '资料': 1, '英文': 1, '本文': 1, '处理': 1, '以': 1, '经过': 1})\n",
            "defaultdict(<class 'int'>, {18: 1, 7: 1, 6: 1, 5: 2, 2: 2, 4: 1, 9: 2, 20: 1, 12: 1, 11: 1, 21: 1, 16: 1, 8: 1, 17: 1, 14: 1, 19: 1, 10: 1, 15: 1, 13: 1, 24: 1, 36: 1, 34: 1, 25: 1, 32: 1, 37: 1, 22: 1, 3: 1, 33: 1, 30: 1, 38: 1, 23: 1, 28: 1, 27: 1, 29: 1, 26: 1, 31: 1, 35: 1})\n",
            "[[4, 6, 11, 2, 7, 12, 13, 8, 9, 2, 10, 14, 4, 15, 7, 2, 10, 5, 16, 17, 8, 4, 6, 18, 19, 20, 21], [22, 23, 24, 25, 26, 27, 3, 2, 28, 5, 29, 30, 31, 32, 3, 33, 5, 34, 9, 2, 3, 35, 36, 37, 3, 38]]\n",
            "[[0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
            "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "{'UNK': 1, '的': 2, '文本': 3, '机器': 4, '，': 5, '学习': 6, '理解': 7, '各种': 8, '原始': 9, '东西': 10, '我': 11, '就是': 12, '把': 13, '变成': 14, '可以': 15, '然后': 16, '再用': 17, '算法': 18, '来': 19, '做': 20, '操作': 21, '网上': 22, '大部分': 23, '都': 24, '是': 25, '处理': 26, '英文': 27, '资料': 28, '本文': 29, '就': 30, '以': 31, '中文': 32, '为例': 33, '将': 34, '经过': 35, '预处理': 36, '得到': 37, '向量': 38}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QX349qyquhTo",
        "colab_type": "code",
        "outputId": "3ca2d028-394b-420d-bc78-ee29157bde53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer1.texts_to_sequences(['你好'])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0OCe1bjlOxg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_f8SaGZkwPjE",
        "colab_type": "code",
        "outputId": "bbbf0fdc-86e3-4455-d014-88cda0825df0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "maxlength = 40\n",
        "pad_sequences(tokenizer.texts_to_sequences(data),\n",
        "              maxlen=maxlength,dtype='int32',\n",
        "              padding='pre',truncating='pre',value=0)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  4,  6, 11,\n",
              "         2,  7, 12, 13,  8,  9,  2, 10, 14,  4, 15,  7,  2, 10,  5, 16,\n",
              "        17,  8,  4,  6, 18, 19, 20, 21],\n",
              "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 22, 23,\n",
              "        24, 25, 26, 27,  3,  2, 28,  5, 29, 30, 31, 32,  3, 33,  5, 34,\n",
              "         9,  2,  3, 35, 36, 37,  3, 38]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXTZ5byYaF51",
        "colab_type": "code",
        "outputId": "a8c12a94-f4da-4168-a8d0-9f5d404734ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# 求平均值\n",
        "data = [\n",
        "        [[1.0,2,3],[3,4,5]],\n",
        "        [[6,8,7],[7,7,7]],\n",
        "        ]\n",
        "# data = np.array(data)\n",
        "x = tf.reduce_mean(data, [1], keepdims=False)\n",
        "# x = tf.reduce_mean(x, [1], keepdims=True)\n",
        "y = [[2,2],[3,3],[4.0,4]]\n",
        "b =[1,2]\n",
        "tf.matmul(x, y)+1"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
              "array([[30. , 30. ],\n",
              "       [64.5, 64.5]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e8-1mb_aZrV",
        "colab_type": "code",
        "outputId": "13ac3b61-c605-4818-b946-4affad80939b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2,), dtype=int64, numpy=array([3, 7])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qaWRnZTwu2W",
        "colab_type": "code",
        "outputId": "2b9f955e-7210-407e-bb2f-cdbe7c944600",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "w_initer = tf.random_normal_initializer()\n",
        "a = tf.Variable(name='w1', \n",
        "                initial_value=w_initer(shape=(2,1),dtype=tf.float16))\n",
        "print(a)\n",
        "b = tf.Variable(name='w1', \n",
        "                initial_value=w_initer(shape=(1,1),dtype=tf.float16))\n",
        "print(b)\n",
        "tf.matmul(a,b)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tf.Variable 'w1:0' shape=(2, 1) dtype=float16, numpy=\n",
            "array([[-0.04288],\n",
            "       [-0.08356]], dtype=float16)>\n",
            "<tf.Variable 'w1:0' shape=(1, 1) dtype=float16, numpy=array([[0.0781]], dtype=float16)>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 1), dtype=float16, numpy=\n",
              "array([[-0.00335 ],\n",
              "       [-0.006527]], dtype=float16)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TF6Txd5lGFyN",
        "colab_type": "code",
        "outputId": "452a2d69-4efe-4166-f272-a8d69471eaff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "!pip install pinyin"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pinyin\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/95/d2969f1071b7bc0afff407d1d7b4b3f445e8e6b59df7921c9c09e35ee375/pinyin-0.4.0.tar.gz (3.6MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6MB 3.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pinyin\n",
            "  Building wheel for pinyin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pinyin: filename=pinyin-0.4.0-cp36-none-any.whl size=3630485 sha256=5540a3c2ffac99f7c868ad3f0a48ff79b586017b2defe92f58aa6537b0c91e2d\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/90/5a/e9844998f3e67b29c5bbca9dc20b5c76f936a45ea77f78e4a9\n",
            "Successfully built pinyin\n",
            "Installing collected packages: pinyin\n",
            "Successfully installed pinyin-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LLqrEV59xy_",
        "colab_type": "code",
        "outputId": "617794f5-16bc-43b4-ddc1-66f8b949e001",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        }
      },
      "source": [
        "# code for NNLM\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pinyin\n",
        "\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "word_sentences = [ \"我 喜  欢 狗 , 但是 我 不 喜欢 买 狗粮\", \n",
        "             \"我 最爱 咖啡 ， 但是 我 得 加 糖\", \n",
        "             \"我 讨厌 牛奶 ， 可是 我 知道 牛奶 有助于 消化\"]\n",
        "# 拼音组成\n",
        "pinyin_sentences = []\n",
        "for sen in word_sentences:\n",
        "  pinyin_sentences.append(' '.join([pinyin.get(word, format=\"strip\", delimiter=\"\") for word in sen.split(\" \")]))\n",
        "\n",
        "\n",
        "sentences = []\n",
        "sentences.extend(word_sentences)\n",
        "sentences.extend(pinyin_sentences)\n",
        "\n",
        "word_list = \" \".join(sentences).split()\n",
        "word_list = list(set(word_list))\n",
        "word_dict = {w: i for i, w in enumerate(word_list)}\n",
        "print(\"word_dict:\", word_dict)\n",
        "number_dict = {i: w for i, w in enumerate(word_list)}\n",
        "print(\"number_dict:\", number_dict)\n",
        "n_class = len(word_dict) # number of Vocabulary\n",
        "\n",
        "# NNLM Parameter\n",
        "n_step = 4 # number of steps ['i like', 'i love', 'i hate']\n",
        "n_hidden = 2 # number of hidden units\n",
        "\n",
        "def make_batch(word_sentences, pinyin_sentences):\n",
        "    input_batch = []\n",
        "    target_batch = []\n",
        "\n",
        "    for word_sen, pinyin_sen in zip(word_sentences, pinyin_sentences):\n",
        "        word = word_sen.split()\n",
        "        pinyin_word = pinyin_sen.split()\n",
        "        for i in range(n_step, len(word)):\n",
        "          input_word = [word_dict[n] for n in word[i-n_step // 2:i]]\n",
        "          input_pinyin = [word_dict[n] for n in pinyin_word[i-n_step //2:i]]\n",
        "          input_word.extend(input_pinyin)\n",
        "          print(input_word)\n",
        "          target = word_dict[word[i]]\n",
        "          input_batch.append(np.eye(n_class)[input_word])\n",
        "          target_batch.append(np.eye(n_class)[target])\n",
        "\n",
        "          # print(\"input:\", input)\n",
        "          # print(\"target:\", target)\n",
        "          # print(\"input_batch\", input_batch)\n",
        "          # print(\"target_batch\", target_batch)\n",
        "\n",
        "    return input_batch, target_batch\n",
        "\n",
        "# Model\n",
        "X = tf.placeholder(tf.float32, [None, n_step, n_class]) # [batch_size, number of steps, number of Vocabulary]\n",
        "Y = tf.placeholder(tf.float32, [None, n_class])\n",
        "\n",
        "input = tf.reshape(X, shape=[-1, n_step * n_class]) # [batch_size, n_step * n_class]\n",
        "H = tf.Variable(tf.random_normal([n_step * n_class, n_hidden]))\n",
        "d = tf.Variable(tf.random_normal([n_hidden]))\n",
        "U = tf.Variable(tf.random_normal([n_hidden, n_class]))\n",
        "b = tf.Variable(tf.random_normal([n_class]))\n",
        "\n",
        "tanh = tf.nn.tanh(d + tf.matmul(input, H)) # [batch_size, n_hidden]\n",
        "model = tf.matmul(tanh, U) + b # [batch_size, n_class]\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=model, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
        "prediction =tf.argmax(model, 1)\n",
        "\n",
        "# Training\n",
        "init = tf.global_variables_initializer()\n",
        "sess = tf.Session()\n",
        "sess.run(init)\n",
        "\n",
        "input_batch, target_batch = make_batch(word_sentences, pinyin_sentences)\n",
        "\n",
        "for epoch in range(15000):\n",
        "    _, loss = sess.run([optimizer, cost], feed_dict={X: input_batch, Y: target_batch})\n",
        "    if (epoch + 1)%1000 == 0:\n",
        "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
        "\n",
        "# Predict\n",
        "def model_predict(word1, word2, word3, word4):\n",
        "  id1 = word_dict[word1]\n",
        "  id2 = word_dict[word2]\n",
        "  id3 = word_dict[word3]\n",
        "  id4 = word_dict[word4]\n",
        "\n",
        "  test_x = []\n",
        "  test_x.append(np.eye(n_class)[[id1, id2, id3, id4]])\n",
        "  predict =  sess.run([prediction], feed_dict={X: test_x})\n",
        "  predict_word = number_dict[predict[0][0]]\n",
        "  print(word1, word2, \"---->\", predict_word)\n",
        "\n",
        "model_predict(\"牛奶\",\"有助于\", \"niunai\", \"youzhuyu\")\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-b5a907ba22d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m word_sentences = [ \"我 喜  欢 狗 , 但是 我 不 喜欢 买 狗粮\", \n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'reset_default_graph'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xiY_pzrl-J8",
        "colab_type": "code",
        "outputId": "a5555bf3-0f8d-44ef-f7a8-872c3bcb6655",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        }
      },
      "source": [
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import zipfile\n",
        "import numpy as np    #在引用包方面，对源码进行了一些修改，因为源码应该是基于python3的 \n",
        "import urllib         #而我使用的python2\n",
        "import tensorflow as tf\n",
        "\n",
        "# Step 1: Download the data.\n",
        "url = 'http://mattmahoney.net/dc/'\n",
        "\n",
        "def maybe_download(filename, expected_bytes):\n",
        "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
        "  if not os.path.exists(filename):\n",
        "    filename, _ = urllib.urlretrieve(url + filename, filename)\n",
        "  statinfo = os.stat(filename)\n",
        "  if statinfo.st_size == expected_bytes:\n",
        "    print('Found and verified', filename)\n",
        "  else:\n",
        "    print(statinfo.st_size)\n",
        "    raise Exception(\n",
        "        'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
        "  return filename\n",
        "\n",
        "filename = maybe_download('text8.zip', 31344016)\n",
        "\n",
        "\n",
        "# Read the data into a list of strings.\n",
        "def read_data(filename):\n",
        "  \"\"\"Extract the first file enclosed in a zip file as a list of words\"\"\"\n",
        "  with zipfile.ZipFile(filename) as f:\n",
        "    data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
        "  return data\n",
        "\n",
        "words = read_data(filename)\n",
        "print('Data size', len(words))\n",
        "\n",
        "# Step 2: Build the dictionary and replace rare words with UNK token.\n",
        "vocabulary_size = 50000\n",
        "\n",
        "def build_dataset(words): #words in corpus , list\n",
        "  count = [['UNK', -1]]\n",
        "  count.extend(collections.Counter(words).most_common(vocabulary_size - 1)) \n",
        "  # 因为使用了most_common,所以count中的word，是按照word在文本中出现的次数从大到小排列的\n",
        "  dictionary = dict()\n",
        "  for word, _ in count:\n",
        "    dictionary[word] = len(dictionary)   # assign id to word\n",
        "  data = list()\n",
        "  unk_count = 0\n",
        "  for word in words:\n",
        "    if word in dictionary:\n",
        "      index = dictionary[word]\n",
        "    else:\n",
        "      index = 0  # dictionary['UNK'] UNK:unknown\n",
        "      unk_count += 1\n",
        "    data.append(index) #translate word to id\n",
        "  count[0][1] = unk_count\n",
        "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
        "  return data, count, dictionary, reverse_dictionary #data:ids count:list of [word,num] dictionary:word->id ,\n",
        "\n",
        "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
        "del words  # Hint to reduce memory.\n",
        "print('Most common words (+UNK)', count[:5])\n",
        "print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n",
        "\n",
        "data_index = 0\n",
        "\n",
        "\n",
        "# Step 3: Function to generate a training batch for the skip-gram model.\n",
        "#skip_skip:从span里面取出多少个word， skip_window:|contex(w)| / 2\n",
        "#span: w上下文的word， 只能从span这个范围中获取\n",
        "def generate_batch(batch_size, num_skips, skip_window):\n",
        "  global data_index\n",
        "  assert batch_size % num_skips == 0\n",
        "  assert num_skips <= 2 * skip_window   # num_skip? skip_window? skip-gram\n",
        "  batch = np.ndarray(shape=(batch_size), dtype=np.int32) #batch_size: the number of words in one batch\n",
        "  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
        "  span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
        "  buffer = collections.deque(maxlen=span) #buffer用来存取w上下文word的id\n",
        "  for _ in range(span):\n",
        "    buffer.append(data[data_index]) # data:ids\n",
        "    data_index = (data_index + 1) % len(data)\n",
        "  for i in range(batch_size // num_skips): #how many num_skips in a batch\n",
        "    target = skip_window  # target label at the center of the buffer\n",
        "    targets_to_avoid = [ skip_window ] # extract the middle word\n",
        "    for j in range(num_skips):\n",
        "      while target in targets_to_avoid:#context中的word，一个只取一次\n",
        "        target = random.randint(0, span - 1)\n",
        "      targets_to_avoid.append(target) #\n",
        "      batch[i * num_skips + j] = buffer[skip_window]\n",
        "      labels[i * num_skips + j, 0] = buffer[target]\n",
        "    buffer.append(data[data_index]) #update the buffer, append the next word to buffer\n",
        "    data_index = (data_index + 1) % len(data)\n",
        "  return batch, labels #batch: ids [batch_size] lebels:ids [batch_size*1]\n",
        "\n",
        "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\n",
        "for i in range(8):\n",
        "  print(batch[i], reverse_dictionary[batch[i]],\n",
        "      '->', labels[i, 0], reverse_dictionary[labels[i, 0]])\n",
        "#========================================================\n",
        "#上面的操作会形成一个这样的输出 batch中存储的是id， 假设我们去skip_size = 4, skip_window = 2\n",
        "#那么，单词 as 所对应的context的word个数就是4个，所以batch中有4个as， 所对应的就是context中的word\n",
        "# 12 as -> 195 term\n",
        "# 12 as -> 5239 anarchism\n",
        "# 12 as -> 6 a\n",
        "# 12 as -> 3084 originated\n",
        "# 6 a -> 12 as\n",
        "# 6 a -> 3084 originated\n",
        "# 6 a -> 2 of\n",
        "# 6 a -> 195 term\n",
        "#=======================================================\n",
        "# Step 4: Build and train a skip-gram model.\n",
        "\n",
        "#在本代码中，batch_size代表的是一个batch中，word的个数，而不是sentense的个数。\n",
        "batch_size = 128\n",
        "embedding_size = 128  # Dimension of the embedding vector.\n",
        "skip_window = 1       # How many words to consider left and right.\n",
        "num_skips = 2         # How many times to reuse an input(buffer) to generate a label.\n",
        "\n",
        "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
        "# validation samples to the words that have a low numeric ID, which by\n",
        "# construction are also the most frequent.\n",
        "valid_size = 16     # Random set of words to evaluate similarity on.\n",
        "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
        "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
        "num_sampled = 64    # Number of negative examples to sample.\n",
        "\n",
        "graph = tf.Graph()\n",
        "\n",
        "with graph.as_default():\n",
        "\n",
        "  # Input data.\n",
        "  #在这里，我们只输入word对应的id，假设batch_size是128,那么我们第一次就输入文本前128个word所对应的id\n",
        "  train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
        "  #labels和inputs是一样的， 只不过一个是行向量（tensor），一个是列向量（tensor）\n",
        "  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
        "  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
        "\n",
        "  # Ops and variables pinned to the CPU because of missing GPU implementation\n",
        "  with tf.device('/cpu:0'):\n",
        "    # Look up embeddings for inputs.\n",
        "    embeddings = tf.Variable(\n",
        "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
        "    # embedding_look 返回值的shape 是 shape(train_inputs)+shape(embeddings)[1:]\n",
        "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
        "\n",
        "    # Construct the variables for the NCE loss\n",
        "    nce_weights = tf.Variable(   #every word has a corresponding nce_weight ad nce_biase\n",
        "        tf.truncated_normal([vocabulary_size, embedding_size],\n",
        "                            stddev=1.0 / math.sqrt(embedding_size)))\n",
        "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-49acc81cd9dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text8.zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m31344016\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-49acc81cd9dd>\u001b[0m in \u001b[0;36mmaybe_download\u001b[0;34m(filename, expected_bytes)\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;34m\"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m   \u001b[0mstatinfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mstatinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_size\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mexpected_bytes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'urllib' has no attribute 'urlretrieve'"
          ]
        }
      ]
    }
  ]
}