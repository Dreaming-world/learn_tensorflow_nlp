{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "RNN_for_text.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMRnkFNBKEQ3h039MX18cRb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dreaming-world/learn_tensorflow_nlp/blob/master/RNN_for_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W34sJ8sIbGeP"
      },
      "source": [
        "# RNN 文本生成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O57ayyajbDjb"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkbWPoAVblDC"
      },
      "source": [
        "# 下载数据集"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYH89J74bj16"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4A9UfBqbtbj",
        "outputId": "c901e608-a83a-41f4-8ba3-67155e5d1216",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 读取并为 py2 compat 解码\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# 文本长度是指文本中的字符个数\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTY1UvVvbwzr",
        "outputId": "f04e4f22-6d8a-4144-a5fe-bce79bf41c81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(text[100])"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGpiRnUicA6I"
      },
      "source": [
        "# 建立字典"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gg1_YTBCb6As",
        "outputId": "31391d1b-6115-4ff0-eff6-8bca9305935b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "print(len(vocab))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "on6LRZ9NcJmd",
        "outputId": "5dfc142a-41bb-4db9-80ec-9ffa7e551691",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "char2id = {u:i for i, u in enumerate(vocab)}\n",
        "id2char = np.array(vocab)\n",
        "print(id2char)\n",
        "text_as_int = [char2id[char] for char in text]\n",
        "print(text_as_int[:100])"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\n' ' ' '!' '$' '&' \"'\" ',' '-' '.' '3' ':' ';' '?' 'A' 'B' 'C' 'D' 'E'\n",
            " 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W'\n",
            " 'X' 'Y' 'Z' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o'\n",
            " 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']\n",
            "[18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 14, 43, 44, 53, 56, 43, 1, 61, 43, 1, 54, 56, 53, 41, 43, 43, 42, 1, 39, 52, 63, 1, 44, 59, 56, 58, 46, 43, 56, 6, 1, 46, 43, 39, 56, 1, 51, 43, 1, 57, 54, 43, 39, 49, 8, 0, 0, 13, 50, 50, 10, 0, 31, 54, 43, 39, 49, 6, 1, 57, 54, 43, 39, 49, 8, 0, 0, 18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 37, 53, 59]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPY-wijLd68O"
      },
      "source": [
        "# 模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmXTGWpCdCRV",
        "outputId": "cb49c48e-60af-41af-9e53-940c017db629",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 设定每个输入句子长度的最大值\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//seq_length\n",
        "\n",
        "# 创建训练样本 / 目标\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(id2char[i.numpy()])\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcpC5h5neSkJ",
        "outputId": "d9568a06-272f-4a2d-b5bd-4d85f01805d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "for item in sequences.take(1):\n",
        "  print(item.numpy())\n",
        "  print(\"\".join(id2char[item.numpy()]))\n",
        "\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
            "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
            " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
            "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
            "  0 37 53 59  1]\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKL9gZccefJC"
      },
      "source": [
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hERK3sqtgWH4",
        "outputId": "fab9dda3-a6b1-4919-c63d-e802f0cac350",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(id2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(id2char[target_example.numpy()])))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_FGiba_gkXE",
        "outputId": "f804fb8a-1803-4835-8342-1aad930399df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 批大小\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# 设定缓冲区大小，以重新排列数据集\n",
        "# （TF 数据被设计为可以处理可能是无限的序列，\n",
        "# 所以它不会试图在内存中重新排列整个序列。相反，\n",
        "# 它维持一个缓冲区，在缓冲区重新排列元素。） \n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int32, tf.int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2r_-90l9gxE2"
      },
      "source": [
        "# 词集的长度\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# 嵌入的维度\n",
        "embedding_dim = 256\n",
        "\n",
        "# RNN 的单元数量\n",
        "rnn_units = 1024"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRY1hrNGg6ts",
        "outputId": "18bc58a9-24c0-4dbc-af33-f4903a282e97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                    batch_input_shape=[BATCH_SIZE, None]))\n",
        "model.add(tf.keras.layers.GRU(rnn_units,\n",
        "                              return_sequences=True,\n",
        "                              stateful=True,\n",
        "                              recurrent_initializer=\"glorot_uniform\"))\n",
        "model.add(tf.keras.layers.Dense(vocab_size))\n",
        "model.summary()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "gru_6 (GRU)                  (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84xU0eqGiA8W",
        "outputId": "dd80e114-0ce0-43bd-dd9a-e07e605f3613",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(target_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[47 52  1 ... 43 56 43]\n",
            " [ 2  0  0 ... 39 50 50]\n",
            " [43  1 41 ... 43 52 58]\n",
            " ...\n",
            " [60 43 52 ... 57  1 47]\n",
            " [60 43  1 ... 50 63  1]\n",
            " [25 43 57 ... 53 61  6]], shape=(64, 100), dtype=int32)\n",
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Urth53HgibmG"
      },
      "source": [
        "model.compile(optimizer=\"adam\",\n",
        "              loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Op8SOJnzizwB",
        "outputId": "ffe3b6b5-dc1d-4173-d5a3-eb0edbccce95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.fit(dataset, epochs=100)\n",
        "model.save_weights(\"rnn.model\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "172/172 [==============================] - 10s 59ms/step - loss: 3.3064 - accuracy: 0.1054\n",
            "Epoch 2/100\n",
            "172/172 [==============================] - 10s 60ms/step - loss: 2.8604 - accuracy: 0.2026\n",
            "Epoch 3/100\n",
            "172/172 [==============================] - 10s 59ms/step - loss: 3.0179 - accuracy: 0.1046\n",
            "Epoch 4/100\n",
            "172/172 [==============================] - 10s 57ms/step - loss: 3.0549 - accuracy: 0.0884\n",
            "Epoch 5/100\n",
            "172/172 [==============================] - 10s 57ms/step - loss: 3.0443 - accuracy: 0.0546\n",
            "Epoch 6/100\n",
            "172/172 [==============================] - 10s 57ms/step - loss: 2.9213 - accuracy: 0.1107\n",
            "Epoch 7/100\n",
            "172/172 [==============================] - 10s 57ms/step - loss: 2.9582 - accuracy: 0.0548\n",
            "Epoch 8/100\n",
            " 42/172 [======>.......................] - ETA: 7s - loss: 2.8681 - accuracy: 0.0589"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4mpa9MWi8L1"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # 评估步骤（用学习过的模型生成文本）\n",
        "\n",
        "  # 要生成的字符个数\n",
        "  num_generate = 1000\n",
        "\n",
        "  # 将起始字符串转换为数字（向量化）\n",
        "  input_eval = [char2id[s] for s in start_string]\n",
        "  print(input_eval)\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "  print(input_eval)\n",
        "\n",
        "  # 空字符串用于存储结果\n",
        "  text_generated = []\n",
        "\n",
        "  # 低温度会生成更可预测的文本\n",
        "  # 较高温度会生成更令人惊讶的文本\n",
        "  # 可以通过试验以找到最好的设定\n",
        "  temperature = 1.0\n",
        "\n",
        "  # 这里批大小为 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # 删除批次的维度\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # 用分类分布预测模型返回的字符\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # 把预测字符和前面的隐藏状态一起传递给模型作为下一个输入\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(id2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvG73FbcjuPn"
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                    batch_input_shape=[1, None]))\n",
        "model.add(tf.keras.layers.GRU(rnn_units,\n",
        "                              return_sequences=True,\n",
        "                              stateful=True,\n",
        "                              recurrent_initializer=\"glorot_uniform\"))\n",
        "model.add(tf.keras.layers.Dense(vocab_size))\n",
        "model.summary()\n",
        "model.load_weights(\"rnn.model\")\n",
        "print(generate_text(model, start_string=u\"ROMEO: \"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ro7chPB1kNUt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}