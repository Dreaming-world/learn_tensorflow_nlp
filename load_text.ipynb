{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "load_text.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dreaming-world/learn_tensorflow_nlp/blob/master/load_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fgZ9gjmPfSnK"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "baYFZMW_bJHh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "061779c4-1af9-487a-8397-cc3559a3d133"
      },
      "source": [
        "!pip install tf-nightly\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import os\n",
        "import numpy as np\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tf-nightly in /usr/local/lib/python3.6/dist-packages (2.2.0.dev20200410)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12.0)\n",
            "Requirement already satisfied: tf-estimator-nightly in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.3.0.dev2020041001)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.9.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.4.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.3.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.10.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.2.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.10.0)\n",
            "Requirement already satisfied: tb-nightly<2.4.0a0,>=2.3.0a0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.3.0a20200409)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.18.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.27.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.34.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.2.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.6.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tf-nightly) (46.1.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (0.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (2.21.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.7.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (3.2.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.6.0.post2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.3.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (3.0.4)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (0.2.8)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (3.1.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YWVWjyIkffau"
      },
      "source": [
        "## 下载数据"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4YlKQthEYlFw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "d74a55ae-ba97-4437-fe3d-dcc0e99d3231"
      },
      "source": [
        "DIRECTORY_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'\n",
        "FILE_NAMES = ['cowper.txt', 'derby.txt', 'butler.txt']\n",
        "\n",
        "for name in FILE_NAMES:\n",
        "  text_dir = tf.keras.utils.get_file(name, origin=DIRECTORY_URL+name)\n",
        "  \n",
        "parent_dir = os.path.dirname(text_dir)\n",
        "\n",
        "parent_dir"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/illiad/cowper.txt\n",
            "819200/815980 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/illiad/derby.txt\n",
            "811008/809730 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/illiad/butler.txt\n",
            "811008/807992 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/root/.keras/datasets'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgmGFJqWTs93",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "e56c38d7-8155-4f4e-e65e-12c8a628cfa9"
      },
      "source": [
        "# 查看每一个文件的具体内容\n",
        "import os\n",
        "\n",
        "for file_name in FILE_NAMES:\n",
        "  file_path = os.path.join(parent_dir, file_name)\n",
        "  show_num = 2\n",
        "  with open(file_path, 'r') as f:\n",
        "    print(file_path)\n",
        "    while show_num > 0:\n",
        "      show_num -= 1\n",
        "      line = f.readline().strip()\n",
        "      print(line)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/.keras/datasets/cowper.txt\n",
            "﻿Achilles sing, O Goddess! Peleus' son;\n",
            "His wrath pernicious, who ten thousand woes\n",
            "/root/.keras/datasets/derby.txt\n",
            "﻿Of Peleus' son, Achilles, sing, O Muse,\n",
            "The vengeance, deep and deadly; whence to Greece\n",
            "/root/.keras/datasets/butler.txt\n",
            "﻿Sing, O goddess, the anger of Achilles son of Peleus, that brought\n",
            "countless ills upon the Achaeans. Many a brave soul did it send\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q3sDy6nuXoNp"
      },
      "source": [
        "## 加载数据到tensorflow格式\n",
        "返回数据的格式为：\n",
        "  (sentence, file_id)\n",
        "第一步：`tf.data.TextLineDataset` 读取文本数据。利用map的方式处理feature和label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K0BjCOpOh7Ch",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "d29ef042-b991-4a79-b4a0-b714a5d7b484"
      },
      "source": [
        "def labeler(example, index):\n",
        "  return example, tf.cast(index, tf.int64)  \n",
        "\n",
        "labeled_data_sets = []\n",
        "\n",
        "for i, file_name in enumerate(FILE_NAMES):\n",
        "  lines_dataset = tf.data.TextLineDataset(os.path.join(parent_dir, file_name)) # 读取数据\n",
        "  labeled_dataset = lines_dataset.map(lambda ex: labeler(ex, i))  # 处理数据的特征与标签，此时数据只有一行数据\n",
        "  labeled_data_sets.append(labeled_dataset)\n",
        "\n",
        "for ele in labeled_data_sets:\n",
        "  for sample in ele.take(5):\n",
        "    print(sample)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(<tf.Tensor: shape=(), dtype=string, numpy=b\"\\xef\\xbb\\xbfAchilles sing, O Goddess! Peleus' son;\">, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'His wrath pernicious, who ten thousand woes'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b\"Caused to Achaia's host, sent many a soul\">, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'Illustrious into Ades premature,'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'And Heroes gave (so stood the will of Jove)'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b\"\\xef\\xbb\\xbfOf Peleus' son, Achilles, sing, O Muse,\">, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'The vengeance, deep and deadly; whence to Greece'>, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'Unnumbered ills arose; which many a soul'>, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'Of mighty warriors to the viewless shades'>, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'Untimely sent; they on the battle plain'>, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'\\xef\\xbb\\xbfSing, O goddess, the anger of Achilles son of Peleus, that brought'>, <tf.Tensor: shape=(), dtype=int64, numpy=2>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'countless ills upon the Achaeans. Many a brave soul did it send'>, <tf.Tensor: shape=(), dtype=int64, numpy=2>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'hurrying down to Hades, and many a hero did it yield a prey to dogs and'>, <tf.Tensor: shape=(), dtype=int64, numpy=2>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'vultures, for so were the counsels of Jove fulfilled from the day on'>, <tf.Tensor: shape=(), dtype=int64, numpy=2>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'which the son of Atreus, king of men, and great Achilles, first fell'>, <tf.Tensor: shape=(), dtype=int64, numpy=2>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M8PHK5J_cXE5"
      },
      "source": [
        "## 拼接三个文本到一起\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6jAeYkTIi9-2",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = 50000\n",
        "BATCH_SIZE = 64\n",
        "TAKE_SIZE = 5000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Qd544E-Sh63L",
        "colab": {}
      },
      "source": [
        "all_labeled_data = labeled_data_sets[0]\n",
        "for labeled_dataset in labeled_data_sets[1:]:\n",
        "  all_labeled_data = all_labeled_data.concatenate(labeled_dataset)\n",
        "\n",
        "# shuffle的功能为打乱dataset中的元素，它有一个参数buffersize，表示打乱时使用的buffer的大小\n",
        "all_labeled_data = all_labeled_data.shuffle(\n",
        "    BUFFER_SIZE, reshuffle_each_iteration=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r4JEHrJXeG5k"
      },
      "source": [
        "使用 `tf.data.Dataset.take` 和 `print` 查看 `(example, label)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gywKlN0xh6u5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "91068314-04e7-4a20-b5ad-42b97d04c3d3"
      },
      "source": [
        "for ex in all_labeled_data.take(5):\n",
        "  print(ex)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(<tf.Tensor: shape=(), dtype=string, numpy=b\"And Priam's offspring, bring disastrous fate?\">, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'On either side: but when the hour was come'>, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'Seeking the chamber of Laodice,'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'The weapons flew; on helm and bossy shield'>, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'at the top of his voice, now from the acropolis, and now speeding up'>, <tf.Tensor: shape=(), dtype=int64, numpy=2>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5rrpU2_sfDh0"
      },
      "source": [
        "## 向量化文本\n",
        "\n",
        "机器学习模型处理的是数字，而不是单词，因此需要将字符串值转换为数字列表。为此，将每个惟一的单词映射到一个惟一的整数\n",
        "\n",
        "### 建立词典\n",
        "\n",
        "### 根据词典索引向量化文本\n",
        "\n",
        "注意; 分词方式保持一致\n",
        "\n",
        "首先，通过将文本标记为单独的单词集合来构建词汇表。在TensorFlow和Python中有几种方法可以做到这一点。本教程:\n",
        "\n",
        "1. Iterate over each example's `numpy` value.\n",
        "2. Use `tfds.features.text.Tokenizer` to split it into tokens.\n",
        "3. Collect these tokens into a Python set, to remove duplicates.\n",
        "4. Get the size of the vocabulary for later use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YkHtbGnDh6mg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "d315ab7b-0b54-4bf7-aa75-b5607d941306"
      },
      "source": [
        "# 根据需求指定分词方法\n",
        "import jieba  # 中文版\n",
        "tokenizer = tfds.features.text.Tokenizer()  # 英文\n",
        "vocabulary_set = set()\n",
        "for text_tensor, _ in all_labeled_data:  # sentence file_id\n",
        "  # some_tokens = tokenizer.tokenize(text_tensor.numpy())\n",
        "  some_tokens = jieba.lcut(text_tensor.numpy().strip())\n",
        "\n",
        "  vocabulary_set.update(some_tokens)\n",
        "\n",
        "vocab_size = len(vocabulary_set)\n",
        "vocab_size"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.871 seconds.\n",
            "Prefix dict has been built successfully.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17186"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivKy2HzHX4tn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b6f57fa3-57f1-499a-b4db-7b5e6e98437e"
      },
      "source": [
        "print(list(vocabulary_set)[:5])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['leopardess', 'Wound', 'extraordinarily', 'murd', 'oxhide']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0W35VJqAh9zs"
      },
      "source": [
        "### 文本向量化\n",
        "\n",
        "第一步：建立词-index 与 index-词的索引表\n",
        "第二部：建立自己的文本向量化方式\n",
        "\n",
        "注意：分词方式与文本向量化时保持一致\n",
        "\n",
        "Create an encoder by passing the `vocabulary_set` to `tfds.features.text.TokenTextEncoder`. The encoder's `encode` method takes in a string of text and returns a list of integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cFZm3r1uz9G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "char2idx = {}\n",
        "char2idx['UNK'] = 0\n",
        "for i,u in enumerate(vocabulary_set):\n",
        "  char2idx[u] = i + 1\n",
        "idx2char = {char2idx[key]:key for key in char2idx.keys()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eN9iQf8wNGa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 指定文本向量化的方式\n",
        "class MyEncoder:\n",
        "  def __init__(self, char2idx):\n",
        "    self.char2idx = char2idx\n",
        "  def encode(self, setence):\n",
        "    word_list = jieba.lcut(setence.strip())\n",
        "    result = []\n",
        "    for ele in word_list:\n",
        "      if ele in self.char2idx:\n",
        "        result.append(self.char2idx[ele])\n",
        "      else:\n",
        "        result.append(self.char2idx[\"UNK\"])\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gkxJIVAth6j0",
        "colab": {}
      },
      "source": [
        "myencoder = MyEncoder(char2idx=char2idx)  # 对应的分词方式为结巴\n",
        "encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)  # 对应的分词方式为tensorflow的token"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v6S5Qyabi-vo"
      },
      "source": [
        "You can try this on a single line to see what the output looks like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jgxPZaxUuTbk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7e099f8e-a0cf-4a09-dd0c-514597818012"
      },
      "source": [
        "# 在单条数据上测试文本向量化\n",
        "example_text = next(iter(all_labeled_data))[0].numpy()\n",
        "print(example_text)\n",
        "# encoded_example = encoder.encode(example_text)\n",
        "encoded_example = myencoder.encode(example_text)\n",
        "print(encoded_example)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b\"And Priam's offspring, bring disastrous fate?\"\n",
            "[312, 8169, 16676, 16221, 11986, 8169, 15664, 1353, 8169, 9185, 8169, 153, 8169, 13031, 17052]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p9qHM0v8k_Mg"
      },
      "source": [
        "Now run the encoder on the dataset by wrapping it in `tf.py_function` and  passing that to the dataset's `map` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HcIQ7LOTh6eT",
        "colab": {}
      },
      "source": [
        "def encode(text_tensor, label):\n",
        "  # encoded_text = encoder.encode(text_tensor.numpy())\n",
        "  encoded_text = myencoder.encode(text_tensor.numpy())\n",
        "  return encoded_text, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eES_Z1ia-Om-"
      },
      "source": [
        "You want to use `Dataset.map` to apply this function to each element of the dataset.  `Dataset.map` runs in graph mode.\n",
        "\n",
        "* Graph tensors do not have a value. \n",
        "* In graph mode you can only use TensorFlow Ops and functions. \n",
        "\n",
        "So you can't `.map` this function directly: You need to wrap it in a `tf.py_function`. The `tf.py_function` will pass regular tensors (with a value and a `.numpy()` method to access it), to the wrapped python function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KmQVsAgJ-RM0",
        "colab": {}
      },
      "source": [
        "def encode_map_fn(text, label):\n",
        "  # py_func doesn't set the shape of the returned tensors.\n",
        "  encoded_text, label = tf.py_function(encode,\n",
        "  inp=[text, label], Tout=(tf.int64, tf.int64))\n",
        "\n",
        "  # `tf.data.Datasets` work best if all components have a shape set\n",
        "  #  so set the shapes manually: \n",
        "  encoded_text.set_shape([None])\n",
        "  label.set_shape([])\n",
        "\n",
        "  return encoded_text, label\n",
        "\n",
        "all_encoded_data = all_labeled_data.map(encode_map_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_YZToSXSm0qr"
      },
      "source": [
        "## 分割数据集为测试集合训练集\n",
        "\n",
        "Use `tf.data.Dataset.take` and `tf.data.Dataset.skip` to create a small test dataset and a larger training set.\n",
        "\n",
        "Before being passed into the model, the datasets need to be batched. Typically, the examples inside of a batch need to be the same size and shape. But, the examples in these datasets are not all the same size — each line of text had a different number of words. So use `tf.data.Dataset.padded_batch` (instead of `batch`) to pad the examples to the same size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r-rmbijQh6bf",
        "colab": {}
      },
      "source": [
        "train_data = all_encoded_data.skip(TAKE_SIZE).shuffle(BUFFER_SIZE)\n",
        "train_data = train_data.padded_batch(BATCH_SIZE, padded_shapes=([None],[]))\n",
        "\n",
        "test_data = all_encoded_data.take(TAKE_SIZE)\n",
        "test_data = test_data.padded_batch(BATCH_SIZE, padded_shapes=([None],[]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fFEHUueEVOxC"
      },
      "source": [
        "Note: As of **TensorFlow 2.2** the `padded_shapes` argument is no longer required. The default behavior is to pad all axes to the longest in the batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0EMSOEPlVOYU",
        "colab": {}
      },
      "source": [
        "train_data = all_encoded_data.skip(TAKE_SIZE).shuffle(BUFFER_SIZE)\n",
        "train_data = train_data.padded_batch(BATCH_SIZE)\n",
        "\n",
        "test_data = all_encoded_data.take(TAKE_SIZE)\n",
        "test_data = test_data.padded_batch(BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xdz7SVwmqi1l"
      },
      "source": [
        "Now, `test_data` and `train_data` are not collections of (`example, label`) pairs, but collections of batches. Each batch is a pair of (*many examples*, *many labels*) represented as arrays.\n",
        "\n",
        "To illustrate:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kMslWfuwoqpB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "357773f8-2983-44db-a6a8-a53f3d60b77f"
      },
      "source": [
        "sample_text, sample_labels = next(iter(test_data))\n",
        "\n",
        "sample_text[0], sample_labels[0]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(30,), dtype=int64, numpy=\n",
              " array([  312,  8169, 16676, 16221, 11986,  8169, 15664,  1353,  8169,\n",
              "         9185,  8169,   153,  8169, 13031, 17052,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0])>, <tf.Tensor: shape=(), dtype=int64, numpy=1>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UI4I6_Sa0vWu"
      },
      "source": [
        "Since we have introduced a new token encoding (the zero used for padding), the vocabulary size has increased by one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K8SUhGFNsmRi"
      },
      "source": [
        "## 建立模型\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QJgI1pow2YR9",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wi0iiKLTKdoF"
      },
      "source": [
        "The first layer converts integer representations to dense vector embeddings. See the [word embeddings tutorial](../text/word_embeddings.ipynb) or more details. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DR6-ctbY638P",
        "colab": {}
      },
      "source": [
        "model.add(tf.keras.layers.Embedding(len(char2idx), 64, mask_zero=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_8OJOPohKh1q"
      },
      "source": [
        "The next layer is a [Long Short-Term Memory](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) layer, which lets the model understand words in their context with other words. A bidirectional wrapper on the LSTM helps it to learn about the datapoints in relationship to the datapoints that came before it and after it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x6rnq6DN_WUs",
        "colab": {}
      },
      "source": [
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cdffbMr5LF1g"
      },
      "source": [
        "Finally we'll have a series of one or more densely connected layers, with the last one being the output layer. The output layer produces a probability for all the labels. The one with the highest probability is the models prediction of an example's label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QTEaNSnLCsv5",
        "colab": {}
      },
      "source": [
        "# One or more dense layers.\n",
        "# Edit the list in the `for` line to experiment with layer sizes.\n",
        "for units in [64, 64]:\n",
        "  model.add(tf.keras.layers.Dense(units, activation='relu'))\n",
        "\n",
        "# Output layer. The first argument is the number of labels.\n",
        "model.add(tf.keras.layers.Dense(3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cb5OBFpxgwTP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "56d44b33-86c8-4342-aa22-a26f53a445da"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 64)          1099968   \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 128)               66048     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 195       \n",
            "=================================================================\n",
            "Total params: 1,178,627\n",
            "Trainable params: 1,178,627\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zLHPU8q5DLi_"
      },
      "source": [
        "Finally, compile the model. For a softmax categorization model, use `sparse_categorical_crossentropy` as the loss function. You can try other optimizers, but `adam` is very common."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pkTBUVO4h6Y5",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DM-HLo5NDhql"
      },
      "source": [
        "## 训练模型\n",
        "\n",
        "This model running on this data produces decent results (about 83%)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aLtO33tNh6V8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "9918be23-cf8a-4569-9df3-87e95986aeeb"
      },
      "source": [
        "model.fit(train_data, epochs=3, validation_data=test_data)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "697/697 [==============================] - 32s 45ms/step - loss: 0.4338 - accuracy: 0.7937 - val_loss: 0.3158 - val_accuracy: 0.8598\n",
            "Epoch 2/3\n",
            "697/697 [==============================] - 30s 43ms/step - loss: 0.2296 - accuracy: 0.9010 - val_loss: 0.3058 - val_accuracy: 0.8622\n",
            "Epoch 3/3\n",
            "697/697 [==============================] - 30s 43ms/step - loss: 0.1670 - accuracy: 0.9305 - val_loss: 0.3509 - val_accuracy: 0.8540\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fea471d9160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KTPCYf_Jh6TH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "686c5fad-e5f8-415c-cd2b-9129515f3a98"
      },
      "source": [
        "eval_loss, eval_acc = model.evaluate(test_data)\n",
        "\n",
        "print('\\nEval loss: {:.3f}, Eval accuracy: {:.3f}'.format(eval_loss, eval_acc))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "79/79 [==============================] - 4s 45ms/step - loss: 0.3509 - accuracy: 0.8540\n",
            "\n",
            "Eval loss: 0.351, Eval accuracy: 0.854\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_Po0X6UnPoE",
        "colab_type": "text"
      },
      "source": [
        "# 主要总结如何读入文本数据"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThSixG_8nR56",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}